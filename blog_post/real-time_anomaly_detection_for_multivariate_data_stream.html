<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>Personal Website [current blog] | Kenneth Odoh's Blog</title>
    <meta http-equiv="x-ua-compatible" content="IE=edge">
    <meta name="author" content="Kenneth Emeka Odoh">
    <meta name="description" content=" The personal website of Kenneth Emeka Odoh">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="icon" type="image/png" href="https://avatars0.githubusercontent.com/u/1905599?s=460&u=facecba98174fef837b6440652f90a2610e00e4c&v=4">
    <script defer src="https://use.fontawesome.com/releases/v5.0.1/js/all.js"></script>
    <link rel="stylesheet" type="text/css" href="/static/style.css">
    <script src="https://code.jquery.com/jquery-3.3.1.min.js" ></script>
    <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
    <script type="text/javascript" src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>

    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML"></script>
    <script type="text/x-mathjax-config">
          MathJax.Hub.Config({
          TeX: {
             equationNumbers: {  autoNumber: "AMS"  },
             //extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
             extensions: ["TeX/AMSmath.js", "TeX/AMSsymbol.js"]
          },
          "HTML-CSS": {
          styles: {
          ".MathJax .mo, .MathJax .mi": {color: "black ! important"}}
          },
          tex2jax: {inlineMath: [['$','$'], ['\\\\(','\\\\)']],processEscapes: true}
          });
    </script> 

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      <style>
      mjx-container[display="block"] {
        display: block;
        margin: 1em 0;
      }
      </style>

      <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js">
      </script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/markdown-it/10.0.0/markdown-it.js">
      </script>

  </head>
  <body class="Site">
    <header class="Site-header">
      <nav class="Nav">
        <div class="Nav-home">
          <a href=/index.html>Kenneth Emeka Odoh</a>
        </div>
        <div class="Nav-sections">
          <a class="Nav-section " href=/index.html >About</a>
          <a class="Nav-section " href=/projects.html>Projects</a>
          <a class="Nav-section Nav-section--active" href=/blogs/1.html>Blogs</a>
          <a class="Nav-section" href=/publications.html>Publications</a>
          <a class="Nav-section" href=/news.html>News</a>
        </div>
      </nav>
      <div class="u-separator"></div>
    </header>
    <div class="Site-content">
      <div class="u-container">

        <div class="Page">
            <article role="main" class="Article">
                <h1 class="Article-title"> Real-Time Anomaly Detection for Multivariate Data Stream </h1>
                <div class="Article-meta">
                  <time datetime=2020-04-07 06:47:41.464543 class="Article-date">07 Apr, 2020</time>
                </div>

                <div class="Article-meta">Tag: machine-learning, signal-processing</div> 


                <section id="text" class="Article-content"> The paper titled “Probabilistic reasoning for streaming anomaly detection” from MIT CSAIL proposed a framework for performing online anomaly detection on univariate data. Unfortunately, most of the data in the real world are multivariate. Hence, mandating the need for more research into performing online anomaly detection in multivariate data. We have been inspired by their work and extend their framework to support multivariate data with some clever optimizations to build a scalable system.

One would be tempted to ask why we have chosen this paper [[1]]() for our study. One answer to this question is that to the best of our knowledge, their work provided a simple framework based on basic statistics to perform real-time anomaly detection of a stream. Furthermore, during my master's thesis, I successfully used a derivation of their work for breaking news detection of an aggregated news stream. Another development that motivated this work is a challenge that I posed during my talk about [time series analysis](https://kenluck2001.github.io/blog_post/pysmooth_a_time_series_library_from_first_principles.html) in Vancouver. I asked the crowd if they can provide suggestions on how to adapt the presented solution to handle multivariate data streams. This is the result of that bet as it is an attempt to complete unfinished work.

The blog will begin by introducing the topic of anomaly detection, followed by a discussion of the original paper [[1]](), and describing extensions of the existing work to handle multivariate data streams. The new formulation that we are proposing would depend on building an online version of the covariance matrix and as such we have provided an implementation of the online covariance matrix, alongside an online inverse covariance matrix based on Sherman–Morrison formula. We have provided a set of mathematical representations and source code.

My implementation of the original paper and the enhanced version of our modified algorithm which is the subject of this blog post can be found in the following links.
- Original paper code: https://github.com/kenluck2001/anomaly
- Blog code:      https://github.com/kenluck2001/anomalyMulti

Furthermore, we have provided a set of detailed experiments on the proposed algorithms in different realistic scenarios. However, we maintain the statistical framework provided by the original paper [[1]]() as it is already tested. We will not fall into the trap of making this writing a survey paper. Hence, we will discuss a few interesting developments in space and as such this manuscript is not expected to be exhaustive. This blog will focus on statistical models and as such, we won't discuss neural networks and their variants in any depth as those would fall outside the scope of this blog post. For more information, see the [paper](https://arxiv.org/abs/1901.03407).

Anomaly detection is the task of classifying patterns that depict abnormal behavior. Therefore, the notion of normal behavior has to be quantified objectively. This concept can be described by several names such as outlier detection, novelty detection, noise detection, and deviation detection. These names are equivalent and would be used interchangeably for the remainder of our monograph. Outliers can arise as a result of human error, equipment error, and faulty systems. Anomaly detection is well-suited for unbalanced data, where the ideal scenario is to predict the behavior of the minority class. There are many applications of anomaly detection in detecting default on loans, fraud detection, and network intrusion detection among others.

There are [different types](http://cucis.ece.northwestern.edu/projects/DMS/publications/AnomalyDetection.pdf) of anomaly which are discussed as follows.
- Point anomaly: This is where a single instance is classified as an anomaly concerning the entire data set. This is ideal for univariate data.
- Contextual Anomaly: when a data instance can be anomalous based on the context (attributes and position in the stream) of the data. This is ideal for multivariate data where for example, in the snapshot reading of a machine, an attribute of a single reading may seem anomalous but can be normal based on consideration of the entire data.
- Collective Anomaly: These are the collection of data that are anomaly as a group, but individually these data points exhibit normal behaviors.

We have summarized many approaches for performing anomaly detection. Our categorization follows loosely the groupings described in [paper](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.109.1943&rep=rep1&type=pdf) is to group existing approaches for anomaly detection
- Unsupervised: This is classifying an outlier with training on unlabelled data.
- Supervised: This is classifying an outlier with training on labeled data.
- Hybrid (mix of both): This is a mix of both schemes. These include semi-supervised learning, self-supervised learning e.t.c.

Anomaly detection algorithms can operate in many settings. This should be carefully thought and be problem-specific.

- Static: These algorithms are designed to work in static datasets. Every item is loaded into memory at a time to perform computation. 
- Online: These algorithms are designed to work in real-time data streams. Items are incrementally loaded into memory. 
- Static + Online: The model can operate in two stages. The initial parameters are estimated in the static setting. Once the parameters are set, as more data arrives, these parameters are incrementally updated. Our extensions and the original work are of this type.

Our discussion will be incomplete if we don't describe how to maintain a collection of the data in the stream to be processed. Hence, we do a quick review of the windows. [Windows](https://www.kdd.org/exploration_files/20-1-Article2.pdf) provide a way to manage data streams. There are several window techniques for streaming analytics:
- Fixed window: This is using a fixed window to store some past information to allow for processing.
- Adaptive window (ADWIN): Keep two windows and drop the former if the past distribution deviates from the current distribution.
- Landmark window: Keep a history of data points that is representative of the distribution of the stream.
- Damped window: This is using a weighting factor on the recent sample and past sample to intensify or dampen the signal. To forget the past, increase the weight of the present.

We have decided to work on anomaly detection algorithms that work in an unsupervised manner. The normal behavior is represented using the PDF of a multivariate normal distribution. Thresholds are set as a way to specify the significance level. The online formulation was used in our work to help the algorithm work even when concept drift occurs. Unsupervised learning provided advantages in cases where getting data with labels can be challenging or even impossible in some contexts. This fits nicely with a data stream when you don't know what to expect from your test distribution.

We will proceed to describe the algorithm in the original paper [[1]]() and then provide extensions in upcoming sections.

### Background work
Anomaly detection algorithm can be aimed at identifying outliers in (any or combination) of the signal changes which may include abrupt transient shift, abrupt distributional shift, and gradual distributional shift [[1]]() which is labeled as "A", "B", and "C" respectively.

![Signal Changes](/static/images/anomaly/timeseries.png)

Online algorithms are useful for real-time applications, as they operate incrementally which is ideal for analyzing the data streams. These algorithms incrementally receive input and make a decision based on an updated parameter that conveys the current state of the data stream. This philosophy contrasts with offline algorithms that assume the entire data is available in memory. The issue with an offline algorithm is that the data may fit in memory. The online algorithm should be both time and space-efficient.

Anomaly detection algorithms may work in diagnosis or accommodation mode [[2]](). The diagnosis method identifies the outlier in the data for further processing of the outlier. The outlier is removed from the data sample so it does not skew the distribution. This is useful when the exact parameters of the distribution are known, so the outlier is excluded from the further estimation of the parameters of the distribution [[2]](). The accommodation method identifies the outliers and uses them for estimating the parameters of the statistical model. This is suitable for data streams that account for the effect of concept drift [[3]]().

Exponential Weighted Moving Average (EWMA) is ideal for keeping a set of running moments in the data stream, but it has some limitations that have led the authors to introduce Probabilistic Exponentially Weighted Moving Average (PEWMA). A single slide from my presentation will clear every misconception between the two algorithms (EWMA and PEWMA) in context.

![PEWMA vs EMWA](/static/images/anomaly/pewma_emwa.png)

PEWMA [[1]]() algorithm works in the accommodation mode. The algorithm allows for concept drift [[3]](), which occurs in data streams by updating the set of parameters that convey the state of the stream. PEWMA [[1]]() suitable as an anomaly detection algorithm that works on an abrupt transient shift, where EWMA fails.

The parameters of the anomaly detection algorithm consist of $X_{t}$ the current data, $\mu_{t}$ the mean of the data, $\hat{X_{t}}$ is the mean of the data, $\hat{\alpha_{t}}$ the current standard deviation, $P_{t}$ the probability density function, $\hat{X_{t+1}}$ the mean of the next data (incremental aggregate), $\hat{\alpha_{t+1}}$ the next standard deviation (incremental aggregates), $T$ the data size, and $t$ a point in $T$. Initialize the process by setting the initial data for training the model $s_{1} = X_{1}$ and $s_{2} = X_{1}^{2}$.

![PEWMA](/static/images/anomaly/original.png)

The processed data is fed to the anomaly detection algorithm with the parameters $\alpha = 0.98, \beta = 0.98$, and $\tau = 0.0044$. The thresholds are chosen for determining outliers that are greater than 3 times the standard deviation in normally distributed data. PEWMA in the original paper was designed to work for point anomaly

##### Hypothesis Testing

Hypothesis is a subjective intuition about the problem. This can be guided by current best practices or transferable skills from adjacent domains. These forms of educated guesses have to be empirically verified to allow your preconceived intuitions to be checked against reality. Let us look at some examples of hypothesis:
- Will this vaccine work on a new virus?
- Will It rain today?

Let us look at the example of a certain school where the Physics teacher is generous with marks. The end of semester report has a class average of 80 with a standard deviation of 15. What is the probability that a student scores
- score < 60 ?
- score > 90 ?

The following code snippet solves the problem as a student scoring less than 60 and greater than 90 has a probability of 0.09 and 0.25 respectively.

```
import math
def cumfunc(mean, sigma, xval):
    """
    @summary: cumulative pdf to the left of the standard normal distribution curve.
    """
    z = (xval - mean) / (sigma * math.sqrt(2))
    y = 0.5 * (1 + math.erf(z))
    return y

if __name__ == '__main__':
    mean = 80; sigma = 15
    x = 60
    res = cumfunc(mean, sigma, x)  # < 60
    print (round(res, 2)) # 0.09

    x = 90
    res = 1 - cumfunc(mean, sigma, x) # > 90
    print (round(res, 2)) # 0.25
```
Let us provide the source code for visualizing the probability of the events described in the code snippet.
```
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm

def contour(plt, score, x_axis, y_axis, colour, lessthan=True):
    x_temp = [x for x in x_axis if x <= score]
    if lessthan:
        y_temp = y_axis[:len(x_temp)]
    else:
        y_temp = y_axis[len(x_temp): ]
        x_temp = [x for x in x_axis if x > score]
    plt.fill_between(x_temp, 0, y_temp, facecolor=colour)

if __name__ == '__main__':
    mean = 80; sd = 15
    x_axis, y_axis = np.arange(10, 140, 0.001), norm.pdf(x_axis,mean,sd)
    plt.plot(x_axis, y_axis); plt.xlabel('Scores'); plt.ylabel('PDF')
    plt.title("Gaussian Distribution with Mean: {} and STD: {}".format(mean, sd))

    colour='#4dac26'; score = 60
    contour(plt, score, x_axis, y_axis, colour, lessthan=True)

    colour='#f1b6da'; score = 90
    contour(plt, score, x_axis, y_axis, colour, lessthan=False)

    plt.show()
```

![Normal Distribution](/static/images/anomaly/chart3.png)

The probability of events (score < 60 and score > 90) as captured by the area of the shaded regions.

If the class average is 80 with a standard deviation of 15, it is with a minuscule probability that a student scores less than 0 or greater than 120. The event categories these outrageous scores can be said to be an anomaly. The scores used in our examples are thresholds. The areas depicting these probabilities can be seen from our chart. 

In summary, the rule of thumb for hypothesis testing can be summarized as follows:
- Identify the Null and Alternative hypothesis.
- Choose a significance level by setting the threshold.
- Decide to reject based on the significance level.

### Extension
Our contribution begins here. We simplify the algorithm by ignoring the details of evolutionary computation in the [paper](http://www.cmap.polytechnique.fr/~nikolaus.hansen/ACECMUaa1p1CMAfES.pdf). The author of the blog post took the premise of evolution as described in the paper to be moving from one generation to the next; as equivalent to moving from one state to another state. This is analogous to how online algorithms work with dynamic changes as new data enters the stream. Cholesky decomposition is used extensively in the algorithms. The  [paper](http://www.cmap.polytechnique.fr/~nikolaus.hansen/ACECMUaa1p1CMAfES.pdf) provided the basis for the online covariance matrix used in this work.

##### Online Covariance matrix
The mathematical formulation can be found here.

1. Estimate covariance matrix for initial data, $X \in R^{n \times m}$.
Initial covariance matrix, $C$ where $C \in R^{n \times m}$, $n$ is the number of samples, $m$ is the number of dimensions.
    
\begin{equation}
C = X * {X}^T
\end{equation}
    
2. Perform Cholesky factorization on the initial covariance matrix, $C$.
\begin{equation}
C_t = A_t * {A_t}^T
\end{equation}
We make use of Scipy's Cholesky decomposition. The input matrix must be positive-definite which means that the eigenvalues are positive which is a requirement for the Cholesky decomposition. For a quick primer on positive-definite, positive semi-definite, and their variants peruse over the [tutorial](https://www.cse.iitk.ac.in/users/rmittal/prev_course/s14/notes/lec11.pdf). The covariance matrix of a multivariate distribution is positive semi-definite. For more insight on how to create a positive semi-definite covariance matrix. Kindly take a look at [discussion board](https://www.researchgate.net/post/How_to_generate_positive-definite_covariance_matrices). The approach taken in this work is to convert to the nearest positive definite matrix. This is sufficient for our purpose. Evaluate and appropriately reapply to your use case. A kind of reasonable approach is just to begin from a random positive definite matrix as a default choice for your covariance.

3. The general form of incremental covariance. This can be best understood that the updated covariance in the presence of new data is equivalent to the weighted average of the past covariance without the new data, and covariance of the transformed input.

\begin{equation}
C_{t+1} = \alpha * C_t + \beta * v_t * {v_t}^T
\end{equation}

Where $v_t = A_t * z_t$ and $z_t \in R^m$ is understood in our implementation is the current data. \alpha and \beta are positive scalar values.

4. Increment the Cholesky factor of the covariance matrix

\begin{equation}
A_{t+1} = \sqrt{\alpha} * A_t + \frac{\sqrt{\alpha}}{\Big\|z_t \Big\|^2} * \left( \sqrt{1 + \frac{\beta * \Big\|z_t \Big\|^2}{\alpha}} - 1 \right) * v_t * z_t
\end{equation}

5. There are difficulties with setting the values of $\alpha$ and $\beta$ respectively. $\alpha + \beta = 1$ as an explicit form of exponential moving average. The author chose to set the values of $\alpha$, $\beta$ using the statistics of the data stream.

The parameters are set as $\alpha = Ca^2$, $\beta = 1 - Ca^2$ and $n$ is the size of the original data in the static settings.
Where $Ca = \sqrt{1 - C_{cov}}$ and $C_{cov} = \frac{2}{{n^2}+6}$.

\begin{equation}
A_{t+1} = Ca * A_t + \frac{Ca}{\Big\|z_t \Big\|^2} * \left( \sqrt{1 + \frac{(1 - Ca^2) * \Big\|z_t \Big\|^2}{Ca^2}} - 1 \right) * v_t * z_t
\end{equation}

The Implementation can be found here    
```
def updateCovariance(alpha, beta, C_t, A_t, z_t):
    """
    @param: alpha, beta, A_t are parameters of the model
    @param: C_t is old covariance matrix, z_t as new data vector
    @return: C_tplus1 is updated covariance matrix
    """
    v_t = np.dot(A_t, z_t.T)
    C_tplus1 = (alpha * C_t)  + (beta * np.matmul(v_t, v_t.T))
    print ("v_t: {}, C_tplus1: {}".format(v_t.shape, C_tplus1.shape))
    return C_tplus1
    
def updateCholeskyFactor(alpha, beta, A_t, z_t):
    """
    @param: alpha, beta, A_t are parameters of the model
    @param: z_t as new data vector
    @return: A_tplus1 is updated covariance matrix
    """
    v_t = np.dot(A_t, z_t.T)
    norm_z = np.linalg.norm(z_t)
    x = math.sqrt(alpha) * A_t
    w = beta * norm_z / alpha
    y = math.sqrt(alpha) * (math.sqrt(1 + w) - 1) * np.dot(v_t, z_t) / norm_z
    A_tplus1 = x + y

    print ("A_t: {}, A_tplus1: {}".format(A_t.shape, A_tplus1.shape))
    return A_tplus1
```

##### Online Inverse Covariance matrix
The mathematical formulation can be found here

1. Estimate covariance matrix for initial data, $X \in R^{n \times m}$.
Initial covariance matrix, $C$ where $C \in R^{n \times m}$, $n$ is the number of samples, $m$ is the number of dimensions.
    
\begin{equation}
C = X * {X}^T
\end{equation}

Inverse the covariance matrix, $C^{-1}$.

\begin{equation}
C^{-1} = \left( X * {X}^T \right)^{-1}
\end{equation}
    
2. Perform Cholesky factorization on initial covariance matrix, $C$.
\begin{equation}
C_t = A_t * {A_t}^T
\end{equation}
We make use of Scipy's Cholesky decomposition.

3. General form of incremental covariance.
This can be best understood that the updated covariance in the presence of new data is equivalent to the weighted average of the past covariance without the new data, and covariance of the transformed input.

\begin{equation}
C_{t+1} = \alpha * C_t + \beta * v_t * {v_t}^T
\end{equation}

Where $v_t = A_t * z_t$ and $z_t \in R^m$ is understood in our implementation is the current data. \alpha and \beta are positive scalar values.

4. Increment the Cholesky factor of the covariance matrix

\begin{equation}
C_{t+1}^{-1} = (\alpha * C_t + \beta * v_t * {v_t}^T)^{-1}
\end{equation}

\begin{equation}
C_{t+1}^{-1} = \alpha^{-1} * (C_t + \frac{\beta * v_t * {v_t}^T}{\alpha})^{-1}
\end{equation}

Let us fix, $\hat{v_t} = \frac{\beta * v_t}{\alpha}$. The resulting simplification using Sherman-Morrison Formula reduces the expression to

\begin{equation}
C_{t+1}^{-1} = \frac{1}{\alpha} * \left({{C_t}^{-1}} - \frac{{{C_t}^{-1}} * \hat{v_t} * {v_t}^T * {{C_t}^{-1}}}{1 + (\hat{v_t} * {{C_t}^{-1}} * {v_t}^T)} \right)
\end{equation}

The Implementation can be found here    
```
def updateInverseCovariance(alpha, beta, invC_t, A_t, z_t):
    """
    @param: alpha, beta, A_t are parameters of the model
    @param: invC_t is old inverse covariance matrix, z_t as new data vector
    @return: invC_tplus1 is updated inverse covariance matrix
    """
    print ("A_t: {}, z_t: {}".format(A_t.shape, z_t.shape))
    v_t = np.dot(A_t, z_t.T)
    hat_vt = (beta * v_t) / alpha

    print ("invC_t: {}, hat_vt: {}, v_t: {}, invC_t: {}".format(invC_t.shape, hat_vt.shape, v_t.shape, invC_t.shape))
    
    y = multi_dot([invC_t, hat_vt, v_t.T, invC_t]) / (1 + multi_dot([hat_vt.T, invC_t, v_t]))
    
    invC_tplus1 = (invC_t - y) / alpha
    print ("invC_tplus1: {}".format(invC_tplus1.shape))
    return invC_tplus1
```

##### Online Multivariate Anomaly Detection

The probability density function makes use of ideas from hypothesis testing. We decide on a threshold which is a confidence level that is used to decide on the acceptance and rejection regions.
1. Use the covariance matrix, $C_{t+1}$ and inverse covariance matrix, ${C_{t+1}}^{-1}$.

2. However, we attempt to increment the mean vector, $\mu$ as new data arrives. It is possible to simplify the Covariance matrix, $C$, which will capture a number of the dynamics of the system. Let $n$ represent the current count of data before new data has arrived.
Also, $\hat{x}$: is the new data, $\mu_{t+1}$: moving average

\begin{equation}
\mu_{t+1} = \frac{(n * \mu_t) + \hat{x}}{n+1}
\end{equation}

3. Set a threshold to determine the acceptance and rejection regions. Items in the acceptance region are considered to be normal behavior.

\begin{equation}
p(x)=\frac{1}{\sqrt{(2\pi)^m|C|}} \exp\left(-\frac{1}{2}(x-\mu)^T{C}^{-1}(x-\mu) \right)
\end{equation}
Where $\mu$ is mean vector, $C$ is covariance matrix, $|C|$ is the determinant of $C$ matrix, $x \in R^{m}$ is data vector, and $m$ is the dimension of $x$ respectively.

The Implementation can be found here    
```
def anomaly(x, mean, cov, threshold=0.001):
    """
    @param: x is the current data vector
    @param: mean is mean vector
    @param: cov is covariance matrix
    @return: score
    """
    score = multivariate_normal.pdf(x, mean=mean, cov=cov)
    return score

def updateMean(mean, z):
    mean_tplus1 = ((n * mean) + z) / (n + 1)
    return mean_tplus1
```
We have provided a clean object-oriented programming based solution with a cleaner API.
```
seed = 0
np.random.seed(seed)
X = np.random.rand(1000,15)
z_t0 = np.random.rand(1,15) # new data

# single case predict
anom = probabilisticMultiEWMA()
anom.init(X)
z_t0 = np.random.rand(1,15) # new data
anom.update(z_t0)
z_t1 = np.random.rand(1,15) # next new data
print ("score: {}".format(anom.predict(z_t1)))
Z = np.random.rand(1000,15)

# Bulk predict
anom = probabilisticMultiEWMA()
anom.init(X)
pred = anom.bulkPredict(Z)
print (pred)
```

### Experiments
We have experimented to evaluate the usefulness of our algorithm by creating a simulation with 10000000 vectors with dimensions of 15. The repeated trial shows that our algorithm is not sensitive to initialization seeds and dimensions of the matrix. This requirement was a deciding factor in the choice of the evaluation metric. More information on the metric will be provided in the Discussion section.

This is to find the trade-off between the static window and the update window. The source code for the experiments can be found [here](https://github.com/kenluck2001/anomalyMulti/blob/master/normal_distribution_charts_with_example.py).

##### Experiment 1
The goal of this experiment is to check the effect of varying the size of the initial static window versus update window

The experiment setup follows loosely the description.
- Split the data into 5 segments
train on 1st segment(static), update covariance on 2nd (online), compare with static covariance - get error

- Train on 1, 2 segment(static), update covariance on 3rd (online), compare with static covariance - get error.

- Train on 1, 2, 3 segment(static), update covariance on 4th (online), compare with static covariance - get error

- Train on 1, 2, 3, 4 segment(static), update covariance on 5th (online), compare with static covariance - get error

![Experiment 1](/static/images/anomaly/chart1.png)

##### Experiment 2

The goal of this experiment is to check the effect of varying the size of the initial static window versus update window

The experiment setup follows loosely the description
- Split the data into 5 segments.

- Train on 1st segment(static), update covariance on remaining segments (2,3,4,5) (online), compare with static covariance - get errors on segments (2,3,4,5)

- Train on 1, 2 segment(static), update covariance on remaining segments (3,4,5) (online), compare with static covariance - get errors on segments (3,4,5)

- Train on 1, 2, 3 segment(static), update covariance on remaining segments (4,5) (online), compare with static covariance - get errors on segments (4,5)

- Train on 1, 2, 3, 4 segment(static), update covariance on remaining segments (5) (online), compare with static covariance - get errors on segments (5)

![Experiment 2](/static/images/anomaly/chart2.png)

### Discussion

Our matrix was flattened to a vector which is used as input. The length of the vector is used to make the loss metric that is agnostic to the dimension of the matrix. The loss function used in the evaluation is Absolute Average Deviation (AAD) because it gives a tighter bound on the error in comparison to MSE or MAD. This is because we take the average of the residuals divided by the ground truth for every sample in our evaluation set. If the residual is close zero, we contribute almost nothing to the measure. However, if the residual is large, we want to know the factor of how large in comparison to the ground truth. This behavior of scaling by the ground truth may explain why this metric tends to be conservative in regression analysis.

\begin{equation}
AAD = \sum_{i=1}^{n} \left| \frac{\hat{Y_i} - Y_i}{Y_i} \right|
\end{equation}

Where $\hat{Y_i}$ is the predicted value, $Y_i$ is ground truth, and $n$ is the length of the flattened matrix.

From our experiments, we can see that building your model with more data in the init (static) phase tends to lead to smaller errors in comparison to having smaller data in the init phase and using more of the data for an update. The observation matches our intuition because when you operate in an online mode, you tend to use smaller storage space, but the trade-off with performance in comparison to batch mode.

The error at the beginning of our training is huge in both charts. This insight shows that rather than performing the expensive operation of converting a covariance matrix to have the property of positive definiteness, it is better to just use random matrices that are positive definite. More data would help us get to convergence as more data arrives.

There are many challenges with anomaly detection methods in modeling the normal behavior of the system. The abnormal behavior shows a deviation from what is the anticipated normal behavior of the system. Many anomaly detections are susceptible to adversarial attacks. In a supervised setting, getting labeled data can be expensive. The definition of noise can be ambiguous.

The success of the experiments has given us the confidence that we would have similar characteristics to the univariate case described in the original paper concerning the enhancement to support the multivariate case.

### Conclusion

There is no generic anomaly detection that works for every possible task. It has to be tuned for your purpose. The underlying assumption in this work is that the features that are used capture significant information about the underlying dynamic of the system. Future work can include extending the multivariate to be production-ready.

### Acknowledgements
I would like to thank my mentor, Dr. Ziyuan Gao from the National University of Singapore for providing technical support during the writing of this manuscript.

### References

- [[1]]() Kevin M. Carter and William W. Streilein. Probabilistic reasoning for streaming anomaly detection. In Proceedings of the Statistical Signal Processing Workshop, pages 377–380, 2012.

- [[2]]() Victoria Hodge and Jim Austin. A survey of outlier detection methodologies. Artificial Intelligence Review, 22(2):85–126, 2004.

- [[3]]() Gregory Ditzler and Robi Polikar. Incremental learning of concept drift from streaming imbalanced data. IEEE Transactions on Knowledge and Data Engineering, 25(10):2283–2301, 2013.

### How to Cite this Article
```
BibTeX Citation

@article{kodoh2020,
  author = {Odoh, Kenneth},
  title = {Real-Time Anomaly Detection for Multivariate Data Stream},
  year = {2020},
  note = {https://kenluck2001.github.io/blog_post/real-time_anomaly_detection_for_multivariate_data_stream}
}
``` </section>

                <table>
                  <tr>
                    <td class="text-muted"> View as: </td>
                    <td> <a class="generate" href="#/pdf/"><img  data-toggle="tooltip" title="Click to download file. Note is not working properly." src=/static/images/pdf_icon.svg style="width: 28px; height: 25px;" alt="pdf here"/></a> </td>
                    <td class="text-muted"> [Alpha version]</td>
                    <td> <p class="download"> <a id="file" target="_blank" ></a> </p> </td>
                  </tr>
                </table>

                <hr class="line"/>
                <div style="display: flex; justify-content: center;">
                    <table>
                      <tr>
                        <td> <a class="left" id="left-9" data-min-index="0" data-max-index="8" href=/blog_post/real-time_anomaly_detection_for_multivariate_data_stream.html><img id="left-0" data-toggle="tooltip" title="Click to previous page." src=/static/images/left.png style="width: 48px; height: 50px;" alt="previous here"/></a> </td>

                        <td> <div class="squares">
                              <div class="icon1">
                                 <!--<p class="text-muted">8/8</p>-->
                                 <p class="text-muted">1/8</p>
                              </div>
                            </div> 
                        </td>
                        
                        <td> <a class="right" id="right-7" data-min-index="0" data-max-index="8" href=/blog_post/paper_summary_lmu_vs_lstm.html><img id="right-1" data-toggle="tooltip" title="Click to next page." src=/static/images/right.png style="width: 48px; height: 50px;" alt="next here"/></a> </td>
                      </tr>
                    </table>
                </div>
                <hr class="line"/>


                <div class="Article-meta">
Continue reading more of our <a href=/blogs/1.html>blog posts</a>, technical <a target="_blank" href=/publications.html#talks>talks</a>, and <a target="_blank" href=/publications.html#publications>publications</a>.
                </div>

                <a target="_blank" data-size="large" class="twitter-share-button"
                  href="https://twitter.com/intent/tweet?url=https://kenluck2001.github.io/blog_post/real-time_anomaly_detection_for_multivariate_data_stream.html&text= Share the blog here">
                 <img src="https://upload.wikimedia.org/wikipedia/fr/c/c8/Twitter_Bird.svg" alt="Smiley face" height="42" width="42">  </a>


            </article>
        </div>

      </div>
    </div>

        <footer class="footer">
            <div class="Footer">
                <p class="text-muted">Icon made by Freepik from www.flaticon.com</p>
                <p class="text-muted">&copy; 2019 Copyright: Kenneth Odoh</p>
            </div>
        </footer>


      <script type="text/javascript" src="/static/js/jquery-1.5.1.min.js"></script>
      <script type="text/javascript" src="/static/js/showdown.js"></script>
      <script type="text/javascript" src="/static/js/pdf.js"></script>
      <script type="text/javascript" src="/static/js/pdf-ui.js"></script>

      <script type="text/javascript" src="/static/js/html2pdf.bundle.min.js"></script>

    <script  type="text/javascript" >

        function animatePaginator(selector) {
            var i = 0;
            $(selector).css('opacity', '1.0');
            setInterval(function () {
                i++;
                if ((i%2)==1)
                {
                    $(selector).fadeOut("slow", function () {
                        $(this).css('opacity', '0.5');
                        $(this).fadeIn("slow");
                    });
                }else{
                    //$(selector).css('opacity', '1.0');

                    $(selector).fadeOut("slow", function () {
                        $(this).css('opacity', '1.0');
                        $(this).fadeIn("slow");
                    });
                }
                i = i % 2
            }, 5000);
        }

        function disablePaginator(selector) {
            $(selector).css('visibility','hidden')
            $(selector).parent().removeAttr("href");
        }


        Array.range = function(a, b, step){
            var A = [];
            if(typeof a == 'number'){
                A[0] = a;
                step = step || 1;
                while(a+step <= b){
                    A[A.length]= a+= step;
                }
            }
            else {
                var s = 'abcdefghijklmnopqrstuvwxyz';
                if(a === a.toUpperCase()){
                    b = b.toUpperCase();
                    s = s.toUpperCase();
                }
                s = s.substring(s.indexOf(a), s.indexOf(b)+ 1);
                A = s.split('');        
            }
            return A;
        }

        var ua = window.navigator.userAgent;
        var isIE = /MSIE|Trident/.test(ua);

        if ( isIE ) {
            //IE specific code goes here
            $(".left").hide()
            $(".right").hide()
        }

        var min_index = $(".left").attr('data-min-index')
        var max_index = $(".left").attr('data-max-index')


        var selectorlist = ['#left', '#right'];
        var indexlist = [min_index, max_index]

        var div;
        var ind;

        // disable pagination
        selector = "#left-" + (parseInt(max_index)+1).toString()
        disablePaginator(selector) 

        selector = "#right-" + min_index  
        disablePaginator(selector) 


        // animate pagination
        for (div of selectorlist) {
                for (ind of Array.range(parseInt(indexlist[0]), parseInt(indexlist[1])+1)) {
                    var selector = div+"-"+ind
                    animatePaginator(selector) 
                } 

        } 




        txt = $('.Article-content').text( );     
        $('.Article-content').html( marked(txt) );

        var md = window.markdownit().use(markdownitFootnote);
        //md.render(/*...*/) // See examples above

        var result = md.render('# markdown-it rulezz!');

    </script >


  </body>
</html>
