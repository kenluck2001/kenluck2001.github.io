<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title> Paper Summary: LMU vs LSTM</title>
    <meta name="author" content="Kenneth Emeka Odoh"/>
    <meta name="description" content="
                                      Both HMM and RNN suffer from disappearing transitions and (vanishing \&amp; exploding) gradient problems, respectively.  LSTM was designed to maintain long time-range dependency on a sequencing task
                                      "/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="icon" type="image/png" href="https://avatars0.githubusercontent.com/u/1905599?s=460&u=facecba98174fef837b6440652f90a2610e00e4c&v=4">

    <meta name="keywords" content="chris, vector, default, learndatascience, machine learning, paper summary, lmu, lstm"/>
    <meta property="og:type" content="article" />
    <meta property="og:title" content=" Paper Summary: LMU vs LSTM" />
    <meta property="og:description" content="
                                              Both HMM and RNN suffer from disappearing transitions and (vanishing \&amp; exploding) gradient problems, respectively.  LSTM was designed to maintain long time-range dependency on a sequencing task
                                              " />

    <script defer src="/static/js/backup/all.js"></script>
    <link rel="stylesheet" type="text/css" href="/static/style.css">
    <script src="/static/js/backup/jquery-3.3.1.min.js" ></script>
    <script src="/static/js/backup/marked.min.js"></script>
    <script type="text/javascript" src="/static/js/backup/bootstrap.min.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML"></script>

    <script type="text/x-mathjax-config">
          MathJax.Hub.Config({
          TeX: {
             equationNumbers: {  autoNumber: "AMS"  },
          },
          "HTML-CSS": {
          styles: {
          ".MathJax .mo, .MathJax .mi": {color: "black ! important"}},
          linebreaks: { automatic: true, width: "container" }
          },
          tex2jax: {inlineMath: [['$','$'], ['\\\\(','\\\\)']],processEscapes: true}
          });
    </script> 

    <style>
      mjx-container[display="block"] {
        display: block;
        margin: 1em 0;
      }
    </style>

  </head>
  <body class="Site">
    <header class="Site-header">
      <nav class="Nav">
        <div class="Nav-home">
          <a href=/index.html>Kenneth Emeka Odoh</a>
        </div>
        <div class="Nav-sections">
          <a class="Nav-section " href=/index.html >About</a>
          <a class="Nav-section " href=/projects.html>Projects</a>
          <a id="blog" class="Nav-section Nav-section--active" href=/blogs/1.html>Blogs</a>
          <a class="Nav-section" href=/publications.html>Publications</a>
          <a class="Nav-section" href=/news.html>News</a>
        </div>
      </nav>
      <div class="u-separator"></div>
    </header>
    <div class="Site-content">
      <div class="u-container">

        <div class="Page">
            <article role="main" class="Article">
                <h1 class="Article-title">  Paper Summary: LMU vs LSTM </h1>
                <div class="Article-meta">
                  <time datetime=2020-01-06 18:39:31.509062 class="Article-date">06 Jan, 2020</time>
                </div>

                <!--<div class="Article-meta">Tag: machine-learning</div> -->
                <div id="tag" class="Article-meta">Tags:
                    
                    <!--<a id=data-tag-machine-learning href=/blogs/machine-learning/1.html>machine-learning</a> -->
                        <a id=machine-learning href=/blogs/machine-learning/1.html>machine-learning</a> 
                    
                </div>
                <section id="text" class="Article-content"> Both HMM and RNN suffer from disappearing transitions and (vanishing \& exploding) gradient problems, respectively. LSTM was designed to maintain long time-range dependency on a sequencing task. However, information flow in the network tends to saturate once the number of time steps exceeds a few thousand. Legendre Memory Unit (LMU) is a revolutionary evolution in RNN design capable of handling extremely long-range dependency. Let's try to figure out why the LMU exceeds the performance of the LSTM in this blog. We proceed by reviewing the paper that introduced LMU.

This is a summary of the paper titled [Legendre Memory Units: Continuous-Time Representation in Recurrent Neural Networks](https://papers.nips.cc/paper/9689-legendre-memory-units-continuous-time-representation-in-recurrent-neural-networks.pdf) that was published in [NeurIPS 2019](https://nips.cc/). This was written for the [Advanced Reading Group](https://www.meetup.com/LearnDataScience/events/wwqpgrybccbmc/) in Vancouver.

Information about the paper are as follows:
- Authors: Aaron R. Voelker, Ivana Kajic, Chris Eliasmith
- Source code:  [Keras implementation of LMU](https://github.com/abr/neurips2019).
- Video: [video link](https://youtu.be/8t64QaTdBcU)

We will not rehash the description of LSTM in this blog as Chris Olah has already given an excellent explanation in his blog post titled [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/). We provide a quick summary of this blog and then focus on understanding LMU.

### LSTM
LSTM makes use of a mix of gating mechanisms and non-linearity to improve on the vanishing and exploding gradient problems that occur in default RNNs. Saturation leads to loss of performance in LSTM, and as a result, several variants of LSTM have been proposed to minimize the effect of saturation.

The likelihood of saturation also increases due to the increased use of squeezing functions such as sigmoid, tanh for their gating effects. This configuration can lead to saturation, which can affect the ability to capture long time-range dependency in a sequence.

The LSTM has the weights of its parameters initialized to random values. This is problematic as the starting point can have a major impact on the quality of the optimization.

### LMU
LMU makes use of less computational resources to maintain long-range time dependencies by decomposing time history in a $d$, number of ODE where the sliding window is represented by using Legendre polynomials with at most $d-1$ degree. LMU can handle extremely long-time range dependency using fewer internal states that can capture the dynamics of the sequence within long time intervals.

Let us rehash the mathematical formulation of LMU and use this knowledge to find out why the LMU is superior in performance to the LSTM.

The cell begins with F(s) as shown in the LMU paper. $\theta$ is a strictly positive scalar value and represents the size of the window.

\begin{equation}
F(s) = e^{-\theta s}
\end{equation}

Taking the natural log of both sides results in

\begin{equation}
ln F(s) = ln e^{-\theta s}
\end{equation}

\begin{equation}
ln F(s) = -\theta s
\end{equation}

The paper assumes that $s$ is equivalent to state vector, $m(t) \in R^{d}$. $\hat{m(t)}$ is updated value of state vector, and $m(t)$ is the current value of state vector.

Equation 1 of the paper shows $\theta \hat{m(t)} = Am(t) + Bu(t)$. This is a common formulation in dynamic system modelling and hence a link to my [blog](https://kenluck2001.github.io/blog_post/pysmooth_a_time_series_library_from_first_principles.html) that discusses Kalman filters.

Ensure that A, B is initialized using Equation 2 in the paper, otherwise, we will not have the desired long-range dependency as this initialization scheme is carefully chosen by the authors. The mathematical origins of the scheme are based on Pade's approximation and Legendre polynomials. This results in an architecture that is less likely to saturate, as the long time-range dependency is preserved even with smaller time steps.

Using the mathematical formulation in the paper alone. Let us try to write a simplistic pseudocode to describe the LMU algorithm in use. With the loss of generality, let us assume a batch size of 1 and the number of iterations set to 1.

- Initialize A, B, m(t)
- Run the code block every epoch
+  for $t$ in $\theta ... n$
  +  for $\hat{\theta}$ in  $t-\theta$ ... $t$    # handle boundaries
     - $u(t - \hat{\theta}) = \sum_{i=0}^{d-1} P_{i} \left( \frac{\hat{\theta}}{\theta}\right) m_{i}(t)$   using $P_{i}$ as one of the basis Legendre polynomial from Equation 3 in paper.
  + $\hat{m(t)} = \frac{Am(t)}{\theta} +  \frac{Bu(t)}{\theta}$
  + $m(t) = \hat{m(t)}$ # update the state vector
  + update $A, B$ until convergence


The performance of the LMU can be tuned by setting the window, $\theta$ and the size ($d$) of the state vector, $m(t)$. By careful tuning of these parameters, we can make a storage space versus performance trade-off on a chaotic time series. For example, higher values of $d$ can increase the capacity of the cell to retain information that spans long time intervals. By contrast, smaller values of $d$ have the opposite effect. The size of the sliding window, $\theta$ is having a similar effect to the parameter, $d$. As a result, $m(t)$ and $\theta$ can be thought of as the memory of the dynamic system.


The LMU unit incorporates an ODE solver. Here is an excellent tutorial on the [ODE](https://jontysinai.github.io/jekyll/update/2019/01/18/understanding-neural-odes.html). The ODE is making a comeback in the neural world. In ResNet, it is implicitly used because it has a structure that is similar to the Euler method for solving ODE. This is because we start with the input, then add gradient residuals at each layer.

### Conclusion

The LMU would serve as a better building block for creating an encode-decoder architecture that would improve sequence-to-sequence modelling tasks. Other structures that are possible with LSTM can also be done with LMU e.g. bidirectional LMU. </section>

                <hr class="line"/>
                <div style="display: flex; justify-content: center;">
                    <table>
                      <tr>
                        <td> <a class="left" id="left-8" data-min-index="0" data-max-index="13" href=/blog_post/real-time_anomaly_detection_for_multivariate_data_stream.html><img id="left-0" data-toggle="tooltip" title="Click to previous page." src=/static/images/left.png style="width: 48px; height: 50px;" alt="previous here"/></a> </td>

                        <td> <div class="squares">
                              <div class="icon1">
                                 <!--<p class="text-muted">7/13</p>-->
                                 <p class="text-muted">7/13</p>
                              </div>
                            </div> 
                        </td>
                        
                        <td> <a class="right" id="right-6" data-min-index="0" data-max-index="13" href=/blog_post/review_of_neurips_%28aabi%29_workshop.html><img id="right-1" data-toggle="tooltip" title="Click to next page." src=/static/images/right.png style="width: 48px; height: 50px;" alt="next here"/></a> </td>
                      </tr>
                    </table>
                </div>
                <hr class="line"/>

                <div style="display: flex; justify-content: center;">
                    <div class="Article-meta">
Read more of our <a href=/blogs/1.html><b>blog posts</b></a>, technical <a target="_blank" href=/publications.html#talks><b>talks</b></a>, and <a target="_blank" href=/publications.html#publications><b>publications</b></a>.
                    </div>

                </div>   


                <!--<div class="Footer">
                <p> Please feel free to <b>donate</b> by clicking <a target="_blank" data-toggle="tooltip" title="Click to make a donation here" href="https://checkout.square.site/pay/a5a1bb5a14e941baae84556f133744cd"><img src=/static/images/donate.png style="width: 40px; height: 40px;" alt="donate here"/></a>using Square Checkout.</p>
                </div>

                <a target="_blank" data-size="large" class="twitter-share-button"
                  href="https://twitter.com/intent/tweet?url=https://kenluck2001.github.io/blog_post/paper_summary_lmu_vs_lstm.html&text= Share the blog here">
                 <img src="https://upload.wikimedia.org/wikipedia/fr/c/c8/Twitter_Bird.svg" alt="Smiley face" height="42" width="42">  </a> -->


            </article>
        </div>

      </div>
    </div>

        <footer class="footer">
            <div class="Footer">
                <p class="text-muted">Icon made by Freepik from www.flaticon.com</p>
                <p class="text-muted">&copy; 2019 Copyright: Kenneth Odoh</p>
            </div>
        </footer>

    <script  type="text/javascript" >

        function animatePaginator(selector) {
            var i = 0;
            $(selector).css('opacity', '1.0');
            setInterval(function () {
                i++;
                if ((i%2)==1)
                {
                    $(selector).fadeOut("slow", function () {
                        $(this).css('opacity', '0.5');
                        $(this).fadeIn("slow");
                    });
                }else{
                    //$(selector).css('opacity', '1.0');

                    $(selector).fadeOut("slow", function () {
                        $(this).css('opacity', '1.0');
                        $(this).fadeIn("slow");
                    });
                }
                i = i % 2
            }, 5000);
        }

        function disablePaginator(selector) {
            $(selector).css('visibility','hidden')
            $(selector).parent().removeAttr("href");
        }


        Array.range = function(a, b, step){
            var A = [];
            if(typeof a == 'number'){
                A[0] = a;
                step = step || 1;
                while(a+step <= b){
                    A[A.length]= a+= step;
                }
            }
            else {
                var s = 'abcdefghijklmnopqrstuvwxyz';
                if(a === a.toUpperCase()){
                    b = b.toUpperCase();
                    s = s.toUpperCase();
                }
                s = s.substring(s.indexOf(a), s.indexOf(b)+ 1);
                A = s.split('');        
            }
            return A;
        }

        var isMobile = { Android: function() { return navigator.userAgent.match(/Android/i); }, BlackBerry: function() { return navigator.userAgent.match(/BlackBerry/i); }, iOS: function() { return navigator.userAgent.match(/iPhone|iPad|iPod/i); }, Opera: function() { return navigator.userAgent.match(/Opera Mini/i); }, Windows: function() { return navigator.userAgent.match(/IEMobile/i) || navigator.userAgent.match(/WPDesktop/i); }, any: function() { return (isMobile.Android() || isMobile.BlackBerry() || isMobile.iOS() || isMobile.Opera() || isMobile.Windows()); }};


        var ua = window.navigator.userAgent;
        var isIE = /MSIE|Trident/.test(ua);

        if ( isIE ) {
            //IE specific code goes here
            $(".left").hide()
            $(".right").hide()
        }

        var min_index = $(".left").attr('data-min-index')
        var max_index = $(".left").attr('data-max-index')


        var selectorlist = ['#left', '#right'];
        var indexlist = [min_index, max_index]

        var div;
        var ind;

        // disable pagination
        selector = "#left-" + (parseInt(max_index)+1).toString()
        disablePaginator(selector) 

        selector = "#right-" + min_index  
        disablePaginator(selector) 


        // animate pagination
        for (div of selectorlist) {
                for (ind of Array.range(parseInt(indexlist[0]), parseInt(indexlist[1])+1)) {
                    var selector = div+"-"+ind
                    animatePaginator(selector) 
                } 

        } 

        // set the search token
        $('#tag').find('a').each(function() {
            var cID = $(this).attr("id"); // Get current url
            $("#"+cID).click(function() {
                localStorage.setItem("tag", cID);
                //alert(cID)
            }); 
        });

        if( isMobile.any() )
        {
            $('#tag').find('a').each(function() {
                var oldUrl = $(this).attr("href"); // Get current url
                var newUrl = oldUrl.replace("blogs", "blogs_mob"); // Create new url
                $(this).attr("href", newUrl); // Set herf value
            });

        }

        $("#blog").click(function() {
            localStorage.setItem("tag", "");
        }); 

        txt = $('.Article-content').text( );     
        $('.Article-content').html( marked.parse(txt) );

    </script >


  </body>
</html>
