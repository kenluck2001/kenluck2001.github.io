<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>Personal Website [current blog] | Kenneth Odoh's Blog</title>
    <meta name="author" content="Kenneth Emeka Odoh">
    <meta name="description" content=" The personal website of Kenneth Emeka Odoh">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="icon" type="image/png" href=/static/images/profile.png>
    <script defer src="https://use.fontawesome.com/releases/v5.0.1/js/all.js"></script>
    <link rel="stylesheet" type="text/css" href="/static/style.css">
    <script src="https://code.jquery.com/jquery-3.3.1.min.js" ></script>
    <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
    <script type="text/javascript" src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>

    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML"></script>
    <script type="text/x-mathjax-config">
          MathJax.Hub.Config({
          TeX: {
             equationNumbers: {  autoNumber: "AMS"  },
             //extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
             extensions: ["TeX/AMSmath.js", "TeX/AMSsymbol.js"]
          },
          "HTML-CSS": {
          styles: {
          ".MathJax .mo, .MathJax .mi": {color: "black ! important"}}
          },
          tex2jax: {inlineMath: [['$','$'], ['\\\\(','\\\\)']],processEscapes: true}
          });
    </script> 
    <!---<script type="text/javascript"
     src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      TeX: {
         equationNumbers: {  autoNumber: "AMS"  },
         extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
      }
    });
    </script> -->

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      <style>
      mjx-container[display="block"] {
        display: block;
        margin: 1em 0;
      }
      </style>
<!---
      <script>
      MathJax = {
        //
        //  Load only TeX input and the contextual menu
        //
        loader: {load: ['input/tex', 'ui/menu']},
        //
        //  When page is ready, render the document
        //
        startup: {pageReady: () => MathJax.startup.document.render()},
        //
        //  Use dollar signs for in-line delimiters in addition to the usual ones
        //
        tex: {inlineMath: {'[+]': [['$', '$']]}},
        //
        //  Override the usual typeset render action with one that generates MathML output
        //
        options: {
          renderActions: {
            typeset: [150,
              //
              //  The function for rendering a document's math elements
              //
              (doc) => {
                const toMML = MathJax.startup.toMML;
                for (math of doc.math) {
                  math.typesetRoot = document.createElement('mjx-container');
                  math.typesetRoot.innerHTML = toMML(math.root);
                  math.display && math.typesetRoot.setAttribute('display', 'block');
                }
              },
              //
              //  The function for rendering a single math expression
              //
              (math, doc) => {
                math.typesetRoot = document.createElement('mjx-container');
                math.typesetRoot.innerHTML = MathJax.startup.toMML(math.root);
                math.display && math.typesetRoot.setAttribute('display', 'block');
              }
            ]
          }
        }
      };
      </script>
      <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js">
      </script>
      <!--<script async src="https://cdnjs.cloudflare.com/ajax/libs/markdown-it-footnote/3.0.2/markdown-it-footnote.js">
      </script>-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/markdown-it/10.0.0/markdown-it.js">
      </script>

  </head>
  <body class="Site">
    <header class="Site-header">
      <nav class="Nav">
        <div class="Nav-home">
          <a href=/index.html>Kenneth Emeka Odoh</a>
        </div>
        <div class="Nav-sections">
          <a class="Nav-section " href=/index.html >About</a>
          <a class="Nav-section " href=/projects.html>Projects</a>
          <a class="Nav-section Nav-section--active" href=/blogs/1.html>Blogs</a>
          <a class="Nav-section" href=/publications.html>Publications</a>
          <a class="Nav-section" href=/news.html>News</a>
        </div>
      </nav>
      <div class="u-separator"></div>
    </header>
    <div class="Site-content">
      <div class="u-container">

        <div class="Page">
            <article role="main" class="Article">
                <h1 class="Article-title"> Fairness meets Adversarial attack </h1>
                <div class="Article-meta">
                  <time datetime=2019-10-05 16:39:51.585270 class="Article-date">05 Oct, 2019</time>
                </div>

                <div class="Article-meta">Tag: machine-learning</div>

                <section class="Article-content"> This article is written as an attempt to provide an intuitive explanation of the connectedness between fairness and adversarial attacks. The concept of fairness and adversarial attacks keeps evolving and as such it is impossible to cover in one blog post.

We will discuss fairness in details and proceed to make a dive into adversarial attacks. We conclude by finding the connection between fairness and adversarial attacks. Following the hackerâ€™s path, it is desirable to learn by connecting the dots between seemingly related concepts. It is a good idea to understand that many aspects of fairness can only be fixed by the legislature. Our goal is to find how both concepts are connected, albeit with analogies, informal proofs and philosophies. 

### Fairness

Machine learning has the potential to worsen social inequalities when misused. This problem can happen due to algorithm bias, data imbalance, and data irregularities. Fairness is an attempt to solve the problem by trying to explain the blind spot in the model as described in [blog](https://jeremykun.com/2015/10/19/one-definition-of-algorithmic-fairness-statistical-parity/). An unacceptable approach to ensuring fairness is to remove racial and demographic-related attributes from the data. While it seems plausible, this information could be redundant in existing features of the data.

Discrimination occurs when a preset metric for fairness varies significantly across different subsets of the population. This is very problematic because the discrimination is due to class membership, rather than individual merits. It can have either a positive or negative context. Positive discrimination is favoritism that is shown to subset of the population e.g affirmative action. While negative discrimination is when a subset of the population is marginalized e.g racism. Conventional wisdom would suggest that providing preferential treatment may guarantee fairness. However, a [paper](https://arxiv.org/pdf/1610.02413.pdf) provides a conflicting proposition that it may even exacerbate the inequality in the society.

There are two major classes of models in machine learning that include discriminative models and generative models. A discriminative classifier is meant to discriminate data points by conditioning the labels on the input, whereas the generative model models the joint distribution between the input and labels.The model has to decide on a set of given data. It should be known that a classifier is only fair to every class in the data, given that there is no significant difference in metric for estimating fairness across the classes in the data set.

At this point, we would like to figure out the connection between uncertainty and fairness.
Uncertainty modeling is a method of explaining aspects of the unknown unknowns of our model, which is the ability to figure out the information content of the blind spot of a machine learning model. Unfairness can occur despite the good intentions of the developer, as it can be manifesting in the blind spot of our algorithm. Aleatoric and epistemic uncertainty are the two common types of uncertainty in Bayesian statistics. Aleatoric uncertainty occurs due to noise in the observation which could be estimated by modeling the noise distribution. Epistemic uncertainty occurs due to missing data which could be estimated by imposing a prior distribution over the parameters of the model, and monitoring the effect of change as the data is varied. [Aleatoric and epistemic uncertainty](https://arxiv.org/pdf/1703.04977.pdf) were combined to learn noise attenuation, thereby improving the neural model. There is a caveat worth making as the noise might be applied uniformly to all the considered population thereby making our best effort not yielding the intended results. Probabilistic numeric is another form of uncertainty due to computation as described in the [paper](https://arxiv.org/pdf/1506.01326.pdf).

The question on how many samples can be estimated is a tricky proposition. This is because we are more likely to have a fairer model if the data is balanced and similar to the probability distribution in the world, so the ideal fairness will be a perfectly balanced data set with equal proportions of sub-classes. However, if the balanced subsets are not diverse and not similar to the world probability, then fairness may not be at the desirable target. As regards to handling unbalanced data, future advances would make feasible the ability to learn from few examples per class using single-shot learning. There are a few open questions that still needs to be answered with regards to sample size. How many additional data points per sub-population does one need to improve fairness of a classifier among the various sub-populations? We would have to borrow ideas from the [paper](https://arxiv.org/pdf/1805.07883.pdf) that places bounds on the sample complexity and then adapted for fairness. A more intuitive question would be to model fairness as a parameter and optimize for it in the loss function as described in [paper](https://arxiv.org/pdf/1803.04383.pdf) where the fairness is defined in the regularization scheme.


#### Evaluation
The scope of fairness can be at the individual level or group level. This provides a realistic way to estimate fairness in a population.

##### Individual level metric
Consistency score is used in literature for measuring relationship between object in cluster analysis. It can also be used for estimating fairness in our context.

[Consistency score](http://www.francescobonchi.com/KDD2016_Tutorial_Part1&2_web.pdf), $\text{C}$, is a measure that determines if similar objects in the population share similar outcomes. This measures fairness at an individual level.

$$  
\text{C}=1-\sum_{i} \sum_{y_{j}\in \text{knn}(y_{i})} |y_{i}-y_{j}|
$$
where $\text{knn}(y_{i})$ is the k nearest neighbors of $y_{i}$

The metric uses a nearest neighbor to each instance of the population and obtains a summation of absolute error between each instance and its nearest neighbors. This has the limitation of setting the number of nearest neighbors as a hyper-parameter. This makes the metric very subjective to the  value of nearest neighbors that was chosen. [KNN](http://www.cs.haifa.ac.il/~rita/ml_course/lectures/KNN.pdf) gives us the ability to capture the characteristics of multi-modal distributions. The rule of thumb for choosing the $K$ in KNN is the $K$ should be large enough to minimize the error and small enough to to capture locality information.

##### Group-level metric
The measure of discrimination in comparison between protected group and unprotected group. This involves quantifying fairness at the level of their membership in different subsets of the population. This gives the average or global measure of the effect.

| group      | denied | granted     |     |
| :---        |    :----:   |    :----:     |   ---:        |
| $protected$     | $a$       | $b$   | $n_{1}$        |
| $unprotected$   | $c$    | $d$      | $n_{2}$              |
| | $m_{1}$ | $m_{2}$       | $n$

Table 1: Benefits (denied vs granted).


$p_{1} = a/n_{1}$, $p_{2} = c/n_{2}$, and $p = m_{1}/n$ respectively.

Risk difference, $\mathrm{RD} = p_{1} - p_{2}$
Risk ratio or relative risk, $\mathrm{RR} = \frac{ p_{1} }{ p_{2} }$
Relative chance, $\mathrm{RC} = \frac{ 1-p_{1} }{ 1-p_{2} }$
Odds ratio = $\frac{ RR }{ RC }$


Risk difference can be used to estimate the difference between the probability of an outcome between the protected and unprotected group of the population. For example in our table, the proportion of the protected population getting denied should be similar to the proportion of proportion of the unprotected subset of the population. This can be extended to population with multiple classes, by comparing the proportion of a class of the population to the absolute value of the average of the proportions per class.


Risk ratio, RR is the ratio of the probability of an outcome in the protected group in comparison to the probability of the same outcome in the unprotected group.

| Score  |      Interpretation      |  
|----------|-------------:|
|RR=1 |  no discrimination between the protected and unprotected group in the population. |
|RR<1 |  protected group is negatively discriminated (punished) against in the population.  |
|RR>1 |  protected group is positively discriminated (favoured) against in the population. |

Table 2: Interpreting the value of Risk ratio.

Relative Chance, RC is the ratio of the probability of an outcome not occurring in the protected group in comparison to the probability of the same outcome not occuring in the unprotected group.


| Score  |      Interpretation      |  
|----------|-------------:|
|RC=1 |  no discrimination between the protected and unprotected group in the population. |
|RC<1 |  protected group is positively discriminated (favoured) against in the population.  |
|RC>1 |  protected group is negatively discriminated (punished) against in the population. |

Table 3: Interpreting the value of Relative chance

Odd is the likelihood that an event would or would not occur. Odds are not the same as probability. For example, odds for 2:1 is equivalent to a probability of $\frac{2}{3}$.
Odd ratio, OR is the ratio of the odds of an outcome in the protected group in comparison to the odd of the same outcome in the unprotected group.


| Score  |      Interpretation      |  
|----------|-------------:|
|OR=1 |  no discrimination between the protected and unprotected group in the population. |
|OR<1 |  protected group is negatively discriminated (punished) against in the population.  |
|OR>1 |  protected group is positively discriminated (favoured) against in the population. |

Table 4: Interpreting the value of Odd ratio.

There are metrics like area under a curve (AUC) and F-score among others. However, these metrics would need to be tuned by altering the thresholds to guarantee certain target values for fairness across different subgroups.


### Adversarial attacks

Machine learning models can be vulnerable to carefully crafted examples designed to fool it. The art of designing countermeasures to enhance the robustness of the model. One school of thought is to treat adversarial attacks as a security threat, while another school of thought is to treat adversarial attacks as a means of improving the robustness of the model.

The consideration for handling adversarial examples must be shaped by business needs as defenses against adversarial attacks can lead to performance degradation and could also lead to loss of intellectual property as shown in the work on [stealing the model](https://arxiv.org/pdf/1609.02943.pdf) served as a web service. The trade-off between performance and stability should be factored to reach a well-informed decision.

There are several [attack strategies](https://arxiv.org/pdf/1810.00069.pdf) that can be used to stage an [adversarial attack](https://arxiv.org/pdf/1810.00069.pdf). This consists of poisoning attack that leads to contamination of the training set thereby compromising the learning algorithm, evasive attack occurs by modifying the testing examples to confuse the system. The settings do not influence the training phase of the model, and exploratory attack requires the black box access to the model by gaining more information of the system and identifying patterns in the training data.

Adversarial attacks can be categorized into white-box and black-box attacks. In white-box setting, the attacker knows the parameters of the model. The attack strategy is based on the gradients with respect to the loss function. This is analogous to chosen-plaintext attack in Cryptography. By passing carefully crafted data set and run through the model to get predicted labels. Whereas in black-box setting, the attacker estimates the parameter of the model by observing the behavior of the network on some carefully made inputs. The attack strategy is based on an approximation of the gradient with respect to observed changes in the input.

There are several defenses for targeted adversarial attacks. However, there is no universal defense for every adversarial attack. Adversarial defenses can prevent a model from compromising the data in which it was trained on. Defense can include data augmentation with original and perturbed data. The technique of distillation has been proposed to protect from the adversarial attacks with some loss in performance.


### Connecting the dots
Fairness can be similar to adversarial attack. Fairness quantities the discrimination which may be intentional or unintentional. Adversarial attacks are usually carefully crafted. However, making the classifier robust to adversarial attacks results in increased gains in addressing edge cases or blind spots in the model. Fairness by design rather than as an afterthought after the building the model. Incorporating fairness in the model can lead to creating more robust models.

The current advances in neural computation are limited in scope for safety-critical systems such as self-driving cars operating at level 5. This motivated the need to ensure neural networks are robust to adversarial examples, while keeping fairness at an acceptable level. Several remedies which have been suggested in the literature for handling adversarial attacks which comprise of augmentations (with a set of transformations), distillation ( for knowledge transfer between models), and many ensemble models to mitigate against adversarial attacks. A known limitation in existing countermeasures for adversarial attacks is the inability to generalize to miscellaneous cases. The adversarial attack could be in the form of corrupting the data by perturbations and thereby biasing the learning algorithm. One possible direction of research is to develop generic countermeasures suitable for computer vision in adversarial settings, which is a step beyond handcrafted [adversarial attacks](https://arxiv.org/pdf/1810.00069.pdf).

Adversarial attacks can be thought as a classifier not fair in some classes which can include the subset of the data that has the adversarial examples. I think the discrimination is not really against the adversarial examples, but against certain subgroups of the population of which the examples are representative. However, an unverified school of thought can assume that the classifier is discriminating against the adversarial examples.

Hence, the weak relationship between fairness and adversarial attacks.

Fairness should be considered from the data collection stage of the processing pipeline. Trying to get a stratified sample of the population. Proceed in form of active learning to gather more data if a class of data has an unacceptable measure for fairness and rebuild your model.

![adversarial attack and fairness](/static/images/adversarial-fairness/fairness-adversarial-attacks.png)

### Acknowledgment

I would like to thank Dr. Ziyuan Gao and Dr. Eugene Yablonsky from the National University of Singapore and Fraser Valley University, BC respectively for providing technical support during the writing of this manuscript. I am grateful to the advanced reading group under the umbrella of [Learn Data Science meetup](https://www.meetup.com/LearnDataScience/) in Vancouver for the lively intellectual conversations on several machine learning topics.

### References
Here are some of the materials that gave me a mental model of the problem.

Tutorial

- https://www.slideshare.net/KrishnaramKenthapadi/fairnessaware-machine-learning-practical-challenges-and-lessons-learned-wsdm-2019-tutorial
- http://www.francescobonchi.com/KDD2016_Tutorial_Part1&2_web.pdf
- https://www.youtube.com/watch?v=jIXIuYdnyyk
- https://docs.google.com/presentation/d/1D7hjxWgv8wYFBAaGIHRu-pW_hq2niFee2me9tfAAg3g/edit#slide=id.g3bfee62da5_0_0
- https://drive.google.com/file/d/1Zseg5dEBFL1Z2emoDFL8TDsnrLvzC2kF/view
- https://drive.google.com/file/d/1rUQkVS0NzSH3IEqZDsczSxBbhYHbjamN/view

Paper

- https://5harad.com/papers/fair-ml.pdf
- https://5harad.com/papers/fairness.pdf
- https://5harad.com/papers/threshold-test.pdf
- http://delivery.acm.org/10.1145/3290000/3287589/p329-Friedler.pdf?ip=174.6.67.20&id=3287589&acc=OPEN&key=4D4702B0C3E38B35%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35%2E6D218144511F3437&__acm__=1551158038_937b36cc03df8cdc9f386f13c9ba662c


News articles

- https://www.nytimes.com/2017/12/20/upshot/algorithms-bail-criminal-justice-system.html
- https://www.washingtonpost.com/news/monkey-cage/wp/2016/10/17/can-an-algorithm-be-racist-our-analysis-is-more-cautious-than-propublicas/?utm_term=.ad9d95b2cd5e

Stability
- http://www.jmlr.org/papers/volume18/17-514/17-514.pdf

Algorithm bias

- https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2886526
- https://arxiv.org/abs/1902.06705
- https://arxiv.org/pdf/1712.07107.pdf
- https://arxiv.org/pdf/1802.05666.pdf </section>

                <div class="Article-meta"><a href=/blogs/1.html> Read more Blogs </a> </div>

                <a target="_blank" data-size="large" class="twitter-share-button"
                  href="https://twitter.com/intent/tweet?url=https://kenluck2001.github.io/blog_post/fairness_meets_adversarial_attack.html&text= Share the blog here">
                 <img src="https://upload.wikimedia.org/wikipedia/fr/c/c8/Twitter_Bird.svg" alt="Smiley face" height="42" width="42">  </a>


            </article>
        </div>


      </div>
    </div>

    <script  type="text/javascript" >
        txt = $('.Article-content').text( );     
        $('.Article-content').html( marked(txt) );

        var md = window.markdownit().use(markdownitFootnote);
        //md.render(/*...*/) // See examples above

        var result = md.render('# markdown-it rulezz!');

    </script >


  </body>
</html>
